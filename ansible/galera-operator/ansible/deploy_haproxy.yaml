- debug:
    msg: "CREATING GALERA HAPROXY"

# refresh pods and services before we create the ConfigMap
- name: lookup k8s service
  set_fact:
    services_lookup: "{{ q('k8s', api_version='v1', kind='Service', namespace=namespace, label_selector=label_selector_value) }}"

- name: lookup pods
  set_fact:
    pods_lookup: "{{ q('k8s', api_version='v1', kind='Pod', namespace=namespace, label_selector=label_selector_value) }}"

# get Service for HAProxy

- block:
  - name: "lookup k8s service for HAProxy"
    set_fact:
      haproxy_service_status: "{{ q('k8s', api_version='v1', kind='Service', namespace=namespace, resource_name='galera-haproxy-service')[0] }}"
  rescue:
  - name: set haproxy_service_status to default on failure
    set_fact:
      haproxy_service_status: []

# create Service for HAProxy if it does not exist
- name: "CREATE SERVICES FOR GALERA HAPROXY"
  k8s:
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: "galera-haproxy-service"
        namespace: "{{ namespace }}"
        labels:
          galera_cluster: "{{ galera_cluster_name }}"
      spec:
        type: NodePort
        selector:
          app: "galera-haproxy"
          galera_cluster: "{{ galera_cluster_name }}"
        ports:
        - name: mysql
          protocol: TCP
          port: 3306
  when: not haproxy_service_status
  register: service_result

- name: "refresh node_service_status for HAProxy"
  set_fact:
    haproxy_service_status: "{{ q('k8s', api_version='v1', kind='Service', namespace=namespace, resource_name='galera-haproxy-service')[0] }}"
  when: service_result|changed

- debug:
    var: haproxy_service_status

# get ConfigMap for HAProxy
- set_fact:
    haproxy_label_selector: "app=galera-haproxy,galera_cluster={{galera_cluster_name}}"

- block:
  - name: "lookup ConfigMap for HAProxy"
    set_fact:
      haproxy_configmap_status: "{{ q('k8s', api_version='v1', kind='ConfigMap', namespace=namespace, label_selector=haproxy_label_selector)[0] }}"
  rescue:
  - name: "set the haproxy config status to default if not found"
    set_fact:
      haproxy_configmap_status: []

# create ConfigMap for HAProxy, if it does not exist or the cluster size changed
- set_fact:
    haproxy_configmap_exists: "{% if haproxy_configmap_status %}True{% else %}False{% endif %}"

- debug:
    var: haproxy_service_status

- set_fact:
    haproxy_config_data: |
          # Load Balancing for Galera Cluster
          listen galera
               bind 0.0.0.0:3306
               balance roundrobin
               mode tcp
               option tcpka
               #option mysql-check user haproxy
               {% for pod in pods_lookup %}{% if pod.status.podIP not in ('', None) %}server {{pod.metadata.name}} {{pod.status.podIP}}:3306 check weight 1
               {% endif %}{% endfor %}

- set_fact:
    haproxy_config_hash: "{{ (haproxy_config_data|hash('md5'))[0:8] }}"

- block:
  - set_fact:
      old_config_hash: "{{ haproxy_configmap_status.Name|replace('galera-haproxy-config-', '') }}"
  rescue:
  - set_fact:
      old_config_hash: ""

- name: "CREATE CONFIGMAP FOR GALERA HAPROXY"
  k8s:
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: "galera-haproxy-config-{{ haproxy_config_hash }}"
        namespace: "{{ namespace }}"
        labels:
          app: "galera-haproxy"
          galera_cluster: "{{ galera_cluster_name }}"
      data:
        haproxy-cfg: "{{ haproxy_config_data }}"
  register: haproxy_configmap_result
  when: haproxy_config_hash != old_config_hash

- name: "DELETE OLD CONFIGMAP FOR HAPROXY"
  k8s:
    state: absent
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: "{{ haproxy_configmap_status.Name }}"
        namespace: "{{ namespace }}"
        labels:
          app: "galera-haproxy"
          galera_cluster: "{{ galera_cluster_name }}"
  when: haproxy_config_hash != old_config_hash and old_config_hash != ""

- name: "refresh haproxy_configmap_status for HAProxy"
  set_fact:
    haproxy_configmap_status: "{{ q('k8s', api_version='v1', kind='ConfigMap', namespace=namespace, label_selector=haproxy_label_selector)[0] }}"
  when: haproxy_configmap_result|changed

- debug:
    var: haproxy_configmap_status

# get Deployment for HAProxy

# create Deployment for HAProxy, if it does not exist

- name: "CREATE DEPLOYMENT FOR GALERA HAPROXY"
  k8s:
    definition:
      kind: Deployment
      apiVersion: v1
      metadata:
        name: "galera-haproxy"
        namespace: "{{ namespace }}"
        labels:
          app: "galera-haproxy"
          galera_cluster: "{{ galera_cluster_name }}"
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: "galera-haproxy"
        template:
          metadata:
            name: "galera-haproxy"
            labels:
              app: "galera-haproxy"
              galera_cluster: "{{ galera_cluster_name }}"
          spec:
            containers:
            - name: "galera-haproxy"
              image: "haproxy:1.8.14"
              restartPolicy: "Always"
              ports:
              - name: mysqld
                containerPort: 3306
              volumeMounts:
              - name: haproxy-cfg-mount
                mountPath: "/usr/local/etc/haproxy"
                readOnly: true
            volumes:
            - name: haproxy-cfg-mount
              configMap:
                name: "galera-haproxy-config-{{ haproxy_config_hash }}"
                items:
                - key: "haproxy-cfg"
                  path: "haproxy.cfg"

# if the cluster size changed and the deployment was not created (but it does exist),
# then delete the pod so the deployment will re-create it with the correct config
